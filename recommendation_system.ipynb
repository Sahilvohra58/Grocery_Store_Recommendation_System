{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**\n",
    "\n",
    "Few of the commonly used libraries in python. Swifter is used to perform parallel processing.\n",
    "\n",
    "(Note that no This notebook can run on a regular CPU in less than 20 minutes, no external GPU is required. However if GPU is avaiable then its even faster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "# !pip install swifter\n",
    "import swifter\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "In this part data from 'transactions.txt' file is parsed in a way that items from each transaction are kept track of in terms of their occurance with each other. In other words their copurchase frequency is counted as the 'transaction.txt' file is preprocessed. \n",
    "\n",
    "However due to memory limitations the file is parced in three parts where copurchase frequency is calculated for each part. \n",
    "\n",
    "The value of *buffer* can be adjusted based on the computer requirements. \n",
    "\n",
    "This whole cell does all the work from reading the file, to parcing, keeping tracking of copurchase frequency and saving the copurchase frequency files for future purposes. \n",
    "\n",
    "Additionally use of functions helps to a big extent in the below cell in terms of memory management as it creates temporary variables. Also, the mutability property of dictionary data types is also helpful to efficient data flow.\n",
    "\n",
    "Note that if you want to run all cells sontinupusly then you can comment the part where it saves the file. The only reason for which the file is saved is because if the anything goes wrong for instance the session crashes or an experiment fails, then we can start from where we left after the saving the files by importing the same files. In that case we do not need to redo this same 15 min cell processing. This also allows us to work with the buffer of anh given size.\n",
    "\n",
    "The cell takes around 14 minutes to perform on a regular CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 500000/500000 [09:59<00:00, 833.45it/s]  \n",
      "Pandas Apply: 100%|██████████| 500000/500000 [07:12<00:00, 1155.42it/s]\n",
      "Pandas Apply: 100%|██████████| 377443/377443 [05:37<00:00, 1118.04it/s]\n"
     ]
    }
   ],
   "source": [
    "copurchase_data = {}\n",
    "\n",
    "all_transactions_path = 'transactions.txt'\n",
    "\n",
    "def update_copurchase_data(rec):\n",
    "  first_item, second_item = rec\n",
    "\n",
    "  if first_item not in copurchase_data:\n",
    "    copurchase_data[first_item] = {}\n",
    "  if second_item not in copurchase_data[first_item]:\n",
    "    copurchase_data[first_item][second_item] = 0\n",
    "\n",
    "  if second_item not in copurchase_data:\n",
    "    copurchase_data[second_item] = {}\n",
    "  if first_item not in copurchase_data[second_item]:\n",
    "    copurchase_data[second_item][first_item] = 0\n",
    "  \n",
    "  copurchase_data[first_item][second_item] += 1\n",
    "  copurchase_data[second_item][first_item] += 1\n",
    "\n",
    "\n",
    "def count_copurchase_frequency(record):\n",
    "  try:\n",
    "    record = [item['item'] for item in eval(record)['itemList']]\n",
    "    purchase_couples = combinations(record,2)\n",
    "\n",
    "    #Cannot wrap a swifter function over another one hence only apply is use here.\n",
    "    pd.Series(purchase_couples).apply(update_copurchase_data)\n",
    "  except:\n",
    "    None\n",
    "\n",
    "\n",
    "def reset_copurchase_data():\n",
    "  global copurchase_data #The global copurchase_data is reset\n",
    "  copurchase_data = {}\n",
    "\n",
    "\n",
    "#alter the value of buffer based on computational requirements.\n",
    "def get_copurchase_frequency(all_transactions_path, buffer = 500000):\n",
    "    start = 0\n",
    "    end = buffer\n",
    "    to_break = False\n",
    "\n",
    "    while not to_break:\n",
    "      \n",
    "      with open(all_transactions_path, 'r') as file:\n",
    "        lines = file.readlines()[start:end]\n",
    "        if len(lines) != buffer:\n",
    "            to_break = True\n",
    "        file.close()\n",
    "\n",
    "      pd.Series(lines).swifter.apply(count_copurchase_frequency) \n",
    "\n",
    "      with open(f'copurchase_data_{start}_to_{end}.json', 'w') as fp:\n",
    "        json.dump(copurchase_data, fp)\n",
    "        fp.close() \n",
    "      \n",
    "      reset_copurchase_data()\n",
    "        \n",
    "      start = end\n",
    "      end = start + buffer\n",
    "\n",
    "get_copurchase_frequency(all_transactions_path) #Takes around 14 minutes to compute on a regular CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reloading Data**\n",
    "\n",
    "This is helpful especiallly in experimantations as we dont need to re-run the preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = []\n",
    "for file in os.listdir():\n",
    "    if file.startswith('copurchase_data') and file.endswith('.json'):\n",
    "        with open(file) as json_file:\n",
    "            full_data.append(json.load(json_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Product Table**\n",
    "\n",
    "The product table originally came in the file 'product.txt'. However it was parsed into a tsv file using the script in 'products.txt' file.\n",
    "\n",
    "Note that the 'Product ID' is made as the index of the table for faster query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = pd.read_csv('all_products.tsv', sep='\\t', index_col= 'Product ID')\n",
    "all_products_idx = all_products.index.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation Matrix**\n",
    "\n",
    "This cell aims to generate 5 recommendations for each of the products in the product table. \n",
    "\n",
    "The product IDs of these 5 recommendations are stored along with the Id from product table in a json file. \n",
    "\n",
    "The reason for json file is the faster query results when its imported as a dictionary.\n",
    "\n",
    "The cell takes around 5 minutes on a regular CPU to perform this operation on all the 70k products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 70771/70771 [03:43<00:00, 316.45it/s] \n"
     ]
    }
   ],
   "source": [
    "copurchase_recommendations = dict(zip(all_products_idx, [None]*len(all_products_idx)))\n",
    "\n",
    "def get_recommendation_matrix(id):\n",
    "    id_lst = [data.get(id, {}) for data in full_data] \n",
    "    id_copurchase = {}\n",
    "    unique_id_lst = []\n",
    "\n",
    "    for data in id_lst:\n",
    "        unique_id_lst = list(set(unique_id_lst + list(data.keys()))) \n",
    "\n",
    "    if len(unique_id_lst) != 0:\n",
    "        def counting(rec):\n",
    "            id_count = 0\n",
    "            for data in id_lst:\n",
    "                id_count += data.get(rec, 0)                \n",
    "            id_copurchase[rec] = id_count\n",
    "        \n",
    "        pd.Series(unique_id_lst).apply(counting)\n",
    "        \n",
    "        id_copurchase = np.array(sorted(id_copurchase.items(), key=lambda x: x[1], reverse=True))[:, 0][:5]\n",
    "\n",
    "        copurchase_recommendations[id] = id_copurchase.tolist()     \n",
    "\n",
    "pd.Series(all_products_idx).swifter.apply(get_recommendation_matrix) #Takes around 5 minutes on a regular CPU\n",
    "with open('recommendation_matrix.json', 'w') as fp:\n",
    "        json.dump(copurchase_recommendations, fp)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Input Function**\n",
    "\n",
    "This is the function that will take an input query of a product ID fom the user. \n",
    "\n",
    "Then it will import the recommendation matrix that was saved in the previoius cell. Since the matrix all the previous cells do not need to be run when quering from user.\n",
    "\n",
    "The function then imports the product table with product ID as its index. it the converts it into a dictionary for faster query results.\n",
    "\n",
    "The function the prints all the recommendations for that product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'Celebration Cupcakes, Chocolate'\n",
      "{'20189092_EA': 'Plastic Bags', '20379763_EA': 'Celebration Cupcakes, White', '20175355001_KG': 'Bananas, Bunch', '20668578_EA': 'PENNY ROUNDING - DO NOT TOUCH', '20812144001_EA': 'Grade A White Eggs, Large'}\n"
     ]
    }
   ],
   "source": [
    "query_ID = '20592676_EA'\n",
    "\n",
    "def get_user_recommendations(query_ID):\n",
    "   with open('recommendation_matrix.json') as json_file:\n",
    "      copurchase_recommendations = json.load(json_file)\n",
    "\n",
    "   all_products_dict = pd.read_csv('all_products.tsv', sep='\\t', index_col= 'Product ID').drop(columns='MCH Category').to_dict()['Item Name']\n",
    "\n",
    "   try:\n",
    "      recommend_ids =  copurchase_recommendations[query_ID]\n",
    "      recommendations = {recommend_id:all_products_dict[recommend_id] for recommend_id in copurchase_recommendations[query_ID]}\n",
    "      print(f\"Recommendations for '{all_products_dict[query_ID]}'\")\n",
    "      print(recommendations)\n",
    "   \n",
    "   except:\n",
    "      print(f'No recommendations for the product.')\n",
    "\n",
    "\n",
    "get_user_recommendations(query_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for '7 Up'\n",
      "{'20175355001_KG': 'Bananas, Bunch', '20801754001_C15': 'Pepsi', '20189092_EA': 'Plastic Bags', '20962518_EA': 'Milk, 2%', '20668578_EA': 'PENNY ROUNDING - DO NOT TOUCH'}\n"
     ]
    }
   ],
   "source": [
    "get_user_recommendations('20801754003_C15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'French Dijon Mustard'\n",
      "{'20175355001_KG': 'Bananas, Bunch', '20127708001_KG': 'Sweet Potatoes', '20130301_EA': 'Montreal Steak Spice', '20143381001_KG': 'Roma Tomatoes', '20055266001_EA': 'Hass Avocado'}\n"
     ]
    }
   ],
   "source": [
    "get_user_recommendations('20000053_EA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment**\n",
    "\n",
    "If this model is kept on production then both the recommendation matrix and the product table can be stored as a variable as hence do not need to be imported. This would allow even faster querying.\n",
    "\n",
    "The function would then just need to lookup into the dictionary for producing fast results. \n",
    "\n",
    "For making the recommendations the only the below mentioned function and the two files that it takes the data from are required.\n",
    "\n",
    "Note that the time taken to evaluate the results is almost 0.1 sec.\n",
    "\n",
    "The production function might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'Parisienne Bread'\n",
      "{'20189092_EA': 'Plastic Bags', '20175355001_KG': 'Bananas, Bunch', '20107500001_EA': 'Green Onion', '20668578_EA': 'PENNY ROUNDING - DO NOT TOUCH', '20143381001_KG': 'Roma Tomatoes'}\n"
     ]
    }
   ],
   "source": [
    "query_ID = '20145949_EA'\n",
    "\n",
    "with open('recommendation_matrix.json') as json_file:\n",
    "      copurchase_recommendations = json.load(json_file)\n",
    "\n",
    "all_products_dict = all_products.drop(columns='MCH Category').to_dict()['Item Name']\n",
    "\n",
    "\n",
    "def get_user_recommendations(query_ID): \n",
    "   try:\n",
    "      recommendations = {recommend_id:all_products_dict[recommend_id] for recommend_id in copurchase_recommendations[query_ID]}\n",
    "      print(f\"Recommendations for '{all_products_dict[query_ID]}'\")\n",
    "      print(recommendations)\n",
    "   \n",
    "   except:\n",
    "      print(f'No recommendations for the product.')\n",
    "\n",
    "   \n",
    "get_user_recommendations(query_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metrics**\n",
    "\n",
    "One of the easy and quick ways to evaluate this model is to check how relevent are the recommended items are to the MCH category of the item for which they are recommended for.\n",
    "\n",
    "So the metrics would be given a list of recommendations for a given item what percentage of them belong to the same MCH category of the the item.\n",
    "\n",
    "There are many outliers like 'Plastic Bags', 'Bananas, Bunch' and 'PENNY ROUNDING - DO NOT TOUCH' which are recommended for many items even though they do not necessarily belong to the same MCH category of the the item they are recommended for. \n",
    "\n",
    "(Since this coding test only asks to propose an evaluation matrics, there is no code for the proposed evaluation metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Better Model (Naive Approach)**\n",
    "\n",
    "A very naive model can be made the recommends items from the same MCH category as the given item. \n",
    "\n",
    "Since this is a naive and very simple model there are many ways to improve this model.\n",
    "\n",
    "The recommendations of this model will be very relevent to the item being queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommendations for 'French Dijon Mustard':\n",
      "['Panda Brand Oyster Sauce', 'Hot Pepper Sauce', 'Sweet Chili Sauce for Chicken', 'Cherry Shiraz Wine Jelly', 'French Garlic Ketchup']\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "mch_categories = pd.read_csv('mch_categories.tsv', sep='\\t', index_col='code')\n",
    "all_products = pd.read_csv('all_products.tsv', sep='\\t', index_col= 'Product ID')\n",
    "\n",
    "def naive_model_user_recommendations(query_ID):\n",
    "    try:\n",
    "        MCH_Category, Item_name = all_products.loc[query_ID, :]\n",
    "        recommendations = sample(all_products[all_products['MCH Category'] == MCH_Category]['Item Name'].tolist(), 5)\n",
    "        print(f\"recommendations for '{Item_name}':\")\n",
    "        print(recommendations)\n",
    "    except:\n",
    "        print(\"No recommendations for this product.\")\n",
    "\n",
    "\n",
    "query_ID = '20000053_EA'\n",
    "naive_model_user_recommendations(query_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommendations for '7 Up':\n",
      "['Mountain Dew Code Red', 'Mandarin Drink', 'Lime', 'Strawberry Malt Beverage', 'Ginger Ale']\n"
     ]
    }
   ],
   "source": [
    "naive_model_user_recommendations('20801754003_C15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommendations for 'Celebration Cupcakes, Chocolate':\n",
      "['Pumpkin Pie', 'Rice Cake, Low Sugar', 'Two-Bite Brownies', 'Lemon Snaps', 'Lemon Rollat']\n"
     ]
    }
   ],
   "source": [
    "naive_model_user_recommendations('20592676_EA')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6dd608ddd16ef8114cec5f050ad31736520bdbacef8568d33dc109fcf0153f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
